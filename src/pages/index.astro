---
import BaseLayout from "../layouts/BaseLayout.astro";
import HorizontalCard from "../components/HorizontalCard.astro";
import { getCollection } from "astro:content";
import createSlug from "../lib/createSlug"

const posts = (await getCollection("blog")).sort((a, b) => b.data.pubDate.valueOf() - a.data.pubDate.valueOf());

const last_posts = posts.slice(0, 3);
---

<BaseLayout sideBarActiveItemID="home">
  <div class="flex flex-col md:flex-row items-center justify-between bg-[#f7f9fb] text-black px-6 py-12 rounded-lg shadow-md">
  <!-- Text Section -->
  <div class="md:w-2/3">
    <div class="text-xl mb-2">Hey there ðŸ‘‹</div>
    <div class="text-5xl font-bold mb-3">I'm Hai Dang Kieu</div>
    <div class="text-2xl font-semibold mb-4">PhD student in Computer Science at VinUniversity and UTS</div>
    <ul class="list-disc list-inside text-lg space-y-1">
      <li>ðŸŽ¯ Recommender Systems (News, Sequential, Multimodal)</li>
      <li>ðŸ§  Graph Neural Networks & Knowledge Graphs</li>
      <li>ðŸ“š Generative Models (LLMs, Prompting)</li>
    </ul>
  </div>

  <!-- Image Section -->
  <div class="md:w-1/3 mt-8 md:mt-0 flex flex-col items-center space-y-4">
    <img src="/logo2.png" alt="VinUniversity Logo 1" class="max-w-[250px] h-auto" />
    <img src="/logo3.png" alt="UTS Logo 2" class="max-w-[250px] h-auto" />
  </div>
</div>


<div class="mt-16">
  <div class="text-3xl w-full font-bold mb-2">My last Publications {"</>"}</div>
</div>
  <HorizontalCard
    title="Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion"
    img="/viral.png"
    desc="Recent advances in multimodal recommendation (MMR) have shown that incorporating rich content sources such as images and text can lead to significant gains representation quality. However, existing methods often rely on coarse visual features and uncontrolled fusion, leading to redundant or misaligned representations. As a result, visual encoders often fail to capture salient, item-relevant semantics, limiting their contribution in multimodal fusion. From an information-theoretic perspective, effective fusion should balance the unique, shared, and redundant information across modalities, preserving complementary cues while avoiding correlation bias. This paper presents VLIF, a vision-language and information-theoretic fusion framework that enhances multimodal recommendation through two key components. (i) A VLM-based visual enrichment module generates fine-grained, title-guided descriptions to transform product images into semantically aligned representations. (ii) An information-aware fusion module, inspired by Partial Information Decomposition (PID), disentangles redundant and synergistic signals across modalities for controlled integration..."
    url="https://arxiv.org/abs/2511.02113"
    badge="Axriv"
    badgeCL="bg-green-600 text-white"
  />
  <div class="divider my-0"></div>
  <HorizontalCard
    title="Enhancing News Recommendation with Hierarchical LLM Prompting"
    img="/Picture3.png"
    desc="Personalized news recommendation systems often struggle to effectively capture the complexity of user preferences, as they rely heavily on shallow representations, such as article titles and abstracts. To address this problem, we introduce a novel method, namely PNR-LLM, for Large Language Models for Personalized News Recommendation. Specifically, PNR-LLM harnesses the generation capabilities of LLMs to enrich news titles and abstracts, and consequently improves recommendation quality. PNR-LLM contains a novel module, News Enrichment via LLMs, which generates deeper semantic information and relevant entities from articles, transforming shallow contents into richer representations. We further propose an attention mechanism to aggregate enriched semantic- and entity-level data, forming unified user and news embeddings that reveal a more accurate user-news match"
    url="https://dl.acm.org/doi/10.1145/3701716.3735085"
    badge="WWW2025"
    badgeCL="bg-green-600 text-white"
  />
  <div class="divider my-0"></div>
  <HorizontalCard
    title="Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User Recommendations"
    img="/kalm4rec.png"
    desc="Recent advancements in Large Language Models (LLMs) have shown significant potential in enhancing recommender systems. However, addressing the cold-start recommendation problem, where users lack historical data, remains a considerable challenge. In this paper, we introduce KALM4Rec (Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User Recommendations), a novel framework specifically designed to tackle this problem by requiring only a few input keywords from users in a practical scenario of cold-start user recommendations. KALM4Rec operates in two main stages: candidates retrieval and LLM-based candidates re-ranking. In the first stage, keyword-driven retrieval models are used to identify potential candidates, addressing LLMs' limitations in processing extensive tokens and reducing the risk of generating misleading information. In the second stage, we employ â€¦"
    url="https://dl.acm.org/doi/abs/10.1145/3701716.3717855"
    badge="LLM4Ecommerce WWW2025"
    badgeCL="bg-green-600 text-white"
  />
  <div class="divider my-0"></div>
  <HorizontalCard
    title="Mi-CGA: Cross-modal Graph Attention Network for robust emotion recognition in the presence of incomplete modalities"
    img="/graph.jpg"
    desc="Multimodal Emotion Recognition in Conversation (Multimodal ERC) is crucial for understanding human communication across various applications. However, the challenge of missing modalities impedes the development of robust models. Existing approaches often overlook scenarios where multiple modalities are absent simultaneously and fail to explore deep semantic interactions between modalities. Additionally, learning high-dimensional interactive features from limited samples is challenging due to missing data. This paper proposes Mi-CGA, a framework tailored for incomplete multimodal learning in conversational contexts. Mi-CGA comprises two main components: Incomplete Multimodal Representation (IMR) and Cross-modal Graph Attention Network (CGA-Net). IMR simulates incomplete modalities, while CGA-Net extracts rich information from conversational graphs. CGA-Net consists of three key â€¦"
    url="https://www.sciencedirect.com/science/article/abs/pii/S0925231225000141"
    badge="NeuroComputing 2025"
    badgeCL="bg-green-600 text-white"
  />
</BaseLayout>

